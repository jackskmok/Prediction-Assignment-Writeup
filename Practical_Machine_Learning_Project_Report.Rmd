---
title: "Project Report of Practical Machine Learning"
author: "by Jack Mok" 
output: html_document
---

## Introduction
This report is for Practical Machine Learning Course Project.

## Data Source 
Training data : https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 

Testing data : https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv 

Source : http://groupware.les.inf.puc-rio.br/har 

## Backgroud 
* In this project, we will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Exploratory Analysis 
```{r, warning=FALSE, quiet = TRUE, echo = FALSE,results='hide',message=FALSE}
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(magrittr)
library(corrplot)
library(randomForest)
set.seed(1234)
```   
### Dataset Overview
* This dataset is licensed under the Creative Commons license (CC BY-SA). Read more: http://groupware.les.inf.puc-rio.br/har#dataset#ixzz4LROCJIrX

* Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

* Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4LRQBWXBv



### Data Loading and Cleaning
```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,result='hide',cache=TRUE}
UrlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
UrlTest  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(UrlTrain))
testing  <- read.csv(url(UrlTest))
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]
dim_trainset <- dim(TrainSet)
dim_testset <- dim(TestSet)
                                        
```

* The dimension of training set is `r dim_trainset[1]` rows x `r dim_trainset[2]` variables.

* The dimension of test set is `r dim_testset[1]` rows x `r dim_testset[2]` variables.

```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,result='hide',cache=TRUE}
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
TrainSet <- TrainSet[, -which(names(TrainSet) %in% remove)]
TestSet <- TestSet[, -which(names(TestSet) %in% remove)]
TestingSet <- testing[, -which(names(testing) %in% remove)] 

NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV] 

NZV <- nearZeroVar(TestSet)
TestSet  <- TestSet[, -NZV]

NZV <- nearZeroVar(TestingSet)
TestingSet  <- TestingSet[, -NZV]

AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]

AllNA    <- sapply(TestSet, function(x) mean(is.na(x))) > 0.95
TestSet  <- TestSet[, AllNA==FALSE]

AllNA    <- sapply(TestingSet, function(x) mean(is.na(x))) > 0.95
TestingSet  <- TestingSet[, AllNA==FALSE]

dim_trainset <- dim(TrainSet)
dim_testset <- dim(TestSet)
dim_TestingSet <- dim(TestingSet)

```

* After remove NA value (Threshold : 95% NA) and the following Unrelevant variables :
 X, 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window'

* The dimension of training set is `r dim_trainset[1]` rows x `r dim_trainset[2]` variables.

* The dimension of test set is `r dim_testset[1]` rows x `r dim_testset[2]` variables.

* The dimension of test case set is `r dim_TestingSet[1]` rows x `r dim_TestingSet[2]` variables.

# Correlation Analysis


```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,cache=TRUE}
corMatrix <- cor(na.omit(TrainSet[sapply(TrainSet, is.numeric)]))
#corrplot.mixed(corMatrix, order = "hclust", tl.col="black", diag = "n", tl.pos = "lt", 
#               lower = "circle", tl.cex = 1, mar=c(1,0,1,0))

corrplot(corMatrix, method = "circle",type="upper",order="hclust")

```

There are 53 predictor variables.  Notice that clusters of higher positively correlated (darker blue) variables along the diagonal; this will also show the groupings of negatively correlated (darker red) variables throughout the dataset. This plot reveals that several groupings of variables with high correlations exist and that principal component analysis (PCA) may provide a suitable data reduction technique.

### Random Forest 
```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,cache=TRUE}
#modFitRF <- train(TrainSet$classe ~., method = "rf", data = TrainSet)
modFitRF <- randomForest(classe ~. , data=TrainSet)
predictRF <- predict(modFitRF, TestSet, type = "class")
confusionRF <- confusionMatrix(predictRF, TestSet$classe)
confusionRF
```
### Decision Tree
```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,cache=TRUE}
#modFitRF <- train(TrainSet$classe ~., method = "rf", data = TrainSet)
modFitRP <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(modFitRP);
predictRP <- predict(modFitRP, TestSet, type = "class")
confusionRP <- confusionMatrix(predictRP, TestSet$classe)
confusionRP
```

### Model Evulation 
* The Accuracy of Random Forest is `r confusionRF$overall[1]`. 
* The Accuracy of Decision Tree is `r confusionRP$overall[1]`. 
* Therefore, using Random forest is more accurate than using Decision Tree.

# Applying Selected Model to the test data 
### Predict Results
```{r, warning=FALSE, quiet = FALSE, echo = FALSE,message=FALSE,cache=TRUE}
predictions <- predict(modFitRF, TestingSet, type = "class")
predictions
``` 